{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6b4f9fa-18c6-4fee-86f9-e4d9ae754b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DESCRIPTION: Extracts data from Ergast F1 API and lands it in Bronze Layer (JSON)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# 1. Configuration\n",
    "# ---------------------------------------------------------\n",
    "base_url = \"http://ergast.com/api/f1\"\n",
    "storage_account_name = \"STORAGE_ACCOUNT\" \n",
    "container_name = \"bronze\"\n",
    "target_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net\"\n",
    "\n",
    "# 2. Helper Function: Fetch All Pages\n",
    "# ---------------------------------------------------------\n",
    "def fetch_ergast_data(endpoint):\n",
    "    \"\"\"\n",
    "    Fetches all records for a specific Ergast API endpoint by handling pagination.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    limit = 100  \n",
    "    offset = 0\n",
    "    \n",
    "    print(f\"\uD83D\uDE80 Starting ingestion for: {endpoint}\")\n",
    "    \n",
    "    while True:\n",
    "        url = f\"{base_url}/{endpoint}.json?limit={limit}&offset={offset}\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"API Error: {response.status_code}\")\n",
    "            \n",
    "        data = response.json()\n",
    "        mr_data = data['MRData']\n",
    "        \n",
    "        # Identify the list key (e.g., 'DriverTable' -> 'Drivers')\n",
    "        if 'DriverTable' in mr_data:\n",
    "            table = mr_data['DriverTable']['Drivers']\n",
    "        elif 'ConstructorTable' in mr_data:\n",
    "            table = mr_data['ConstructorTable']['Constructors']\n",
    "        elif 'CircuitTable' in mr_data:\n",
    "            table = mr_data['CircuitTable']['Circuits']\n",
    "        elif 'RaceTable' in mr_data:\n",
    "            table = mr_data['RaceTable']['Races']\n",
    "        else:\n",
    "            print(f\"⚠️ specific table key not found for {endpoint}\")\n",
    "            break\n",
    "\n",
    "        if not table:\n",
    "            break \n",
    "            \n",
    "        all_results.extend(table)\n",
    "        \n",
    "        # Check if we need to fetch more\n",
    "        total = int(mr_data['total'])\n",
    "        offset += limit\n",
    "        print(f\"   Fetched {len(all_results)} / {total} records...\")\n",
    "        \n",
    "        if offset >= total:\n",
    "            break\n",
    "            \n",
    "    print(f\"✅ Finished. Total records: {len(all_results)}\")\n",
    "    return all_results\n",
    "\n",
    "# 3. Execution: Ingest Key Tables\n",
    "# ---------------------------------------------------------\n",
    "endpoints = [\"drivers\", \"constructors\", \"circuits\", \"races\"]\n",
    "\n",
    "for entity in endpoints:\n",
    "    raw_data = fetch_ergast_data(entity)\n",
    "    \n",
    "    # B. Convert to Spark DataFrame\n",
    "    rdd = spark.sparkContext.parallelize([json.dumps(r) for r in raw_data])\n",
    "    df = spark.read.json(rdd)\n",
    "    \n",
    "    # C. Add Audit Columns\n",
    "    df_with_audit = df.withColumn(\"ingestion_date\", lit(datetime.now())) \\\n",
    "                      .withColumn(\"source_system\", lit(\"Ergast API\"))\n",
    "    \n",
    "    # D. Write to Bronze (Landing Zone)\n",
    "    save_path = f\"{target_path}/{entity}\"\n",
    "    df_with_audit.write.mode(\"overwrite\").format(\"json\").save(save_path)\n",
    "    print(f\"\uD83D\uDCBE Saved {entity} to {save_path}\\n\")\n",
    "\n",
    "print(\"\uD83C\uDF89 API Ingestion Pipeline Completed!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_api_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}